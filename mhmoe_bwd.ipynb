{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n",
    "@triton.jit\n",
    "def leaky_relu(x):\n",
    "    return tl.where(x >= 0, x, 0.01 * x)\n",
    "\n",
    "@triton.jit\n",
    "def d_leacky_relu_inv_backward(x):\n",
    "    return tl.where(x >= 0, 1.0, 0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        # triton.Config({'BLOCK_SIZE_B': 16, 'BLOCK_SIZE_E': 16}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_B': 16, 'BLOCK_SIZE_E': 32}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_E': 16}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_B': 16, 'BLOCK_SIZE_E': 64}, num_stages=2, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_E': 32}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_E': 32}, num_stages=4, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_B': 32, 'BLOCK_SIZE_E': 64}, num_stages=3, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_E': 64}, num_stages=3, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_E': 64, 'GROUP_SIZE_B': 1}, num_stages=2, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_B': 64, 'BLOCK_SIZE_E': 128, 'GROUP_SIZE_B': 1}, num_stages=2, num_warps=4),\n",
    "        # triton.Config({'BLOCK_SIZE_B': 128, 'BLOCK_SIZE_E': 128}, num_stages=2, num_warps=4),\n",
    "    ],\n",
    "    key=['B', 'D', 'E'],\n",
    ")\n",
    "@triton.jit\n",
    "def mlp_wide_kernel_bwd1(\n",
    "    x_ptr, w1_ptr, w2_ptr, o_ptr, dx_ptr, dw1_ptr, dw2_ptr, do_ptr,\n",
    "    B, D: tl.constexpr, E,\n",
    "    stride_xb, stride_xd,\n",
    "    stride_w1d, stride_w1e,\n",
    "    stride_w2e, stride_w2d,\n",
    "    stride_ob, stride_od,\n",
    "    stride_dxb, stride_dxd,\n",
    "    stride_dw1d, stride_dw1e,\n",
    "    stride_dw2e, stride_dw2d,\n",
    "    stride_dob, stride_dod,\n",
    "    BLOCK_SIZE_B: tl.constexpr, BLOCK_SIZE_E: tl.constexpr,\n",
    "    ACTIVATION: tl.constexpr\n",
    "):\n",
    "    \"\"\"Kernel for computing the mlp\n",
    "    Z = X @ W1, H = f(Z), O = H @ W2\n",
    "    - X has shape (B, D)\n",
    "    - W1 has shape (D, E)\n",
    "    - W2 has shape (E, D)\n",
    "    - O has shape (B, D)\n",
    "    - dX has shape (B, D)\n",
    "    - dW1 has shape (D, E)\n",
    "    - dW2 has shape (E, D)\n",
    "    - dO has shape (B, D)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    pid_e = tl.program_id(axis=0)\n",
    "    # batch_groups = tl.cdiv(B, BLOCK_SIZE_B)\n",
    "    # pid_b = pid % batch_groups\n",
    "    # pid_h = pid // batch_groups\n",
    "    # tl.device_print('pid_H', pid_h)\n",
    "    # tl.device_print('pid_B', pid_b)\n",
    "    # pid_d = tl.program_id(axis=1)\n",
    "    TARGET_TYPE = x_ptr.type.element_ty\n",
    "\n",
    "    offs_b = tl.arange(0, BLOCK_SIZE_B)\n",
    "    offs_d = tl.arange(0, D)\n",
    "    offs_e = tl.arange(0, BLOCK_SIZE_E)\n",
    "\n",
    "    x_ptrs = x_ptr + (offs_b[:, None] * stride_xb + offs_d[None, :] * stride_xd)\n",
    "    dx_ptrs = dx_ptr + (offs_b[:, None] * stride_dxb + offs_d[None, :] * stride_dxd)\n",
    "    do_ptrs = do_ptr + (offs_b[:, None] * stride_dob + offs_d[None, :] * stride_dod)\n",
    "\n",
    "\n",
    "    w1_ptrs = w1_ptr + (offs_d[:, None] * stride_w1d + (offs_e[None, :] + pid_e * BLOCK_SIZE_E) * stride_w1e)\n",
    "    w1_mask = (offs_d[:, None] < D) & (offs_e[None, :] < E - pid_e * BLOCK_SIZE_E)\n",
    "    w2_ptrs = w2_ptr + ((offs_e[:, None] + pid_e * BLOCK_SIZE_E) * stride_w2e + offs_d[None, :] * stride_w2d)\n",
    "    w2_mask = (offs_e[:, None] < E - pid_e * BLOCK_SIZE_E) & (offs_d[None, :] < D)\n",
    "\n",
    "    dw1_ptrs = dw1_ptr + (offs_d[:, None] * stride_dw1d + (offs_e[None, :] + pid_e * BLOCK_SIZE_E) * stride_dw1e)\n",
    "    dw1_mask = (offs_d[:, None] < D) & (offs_e[None, :] < E - pid_e * BLOCK_SIZE_E)\n",
    "    dw2_ptrs = dw2_ptr + ((offs_e[:, None] + pid_e * BLOCK_SIZE_E) * stride_dw2e + offs_d[None, :] * stride_dw2d)\n",
    "    dw2_mask = (offs_e[:, None] < E - pid_e * BLOCK_SIZE_E) & (offs_d[None, :] < D)\n",
    "\n",
    "    w1 = tl.load(w1_ptrs, mask=w1_mask, other=0.0)                      # D, BLOCK_SIZE_E\n",
    "    w2 = tl.load(w2_ptrs, mask=w2_mask, other=0.0)                      # BLOCK_SIZE_E, D\n",
    "    dw1 = tl.zeros((D, BLOCK_SIZE_E), dtype=tl.float32)                 # D, BLOCK_SIZE_E\n",
    "    dw2 = tl.zeros((BLOCK_SIZE_E, D), dtype=tl.float32)                 # BLOCK_SIZE_E, D\n",
    "    for b in range(0, tl.cdiv(B, BLOCK_SIZE_B)):\n",
    "\n",
    "        x_mask = (offs_b[:, None] < B - b * BLOCK_SIZE_B) & (offs_d[None, :] < D)\n",
    "        dx_mask = (offs_b[:, None] < B - b * BLOCK_SIZE_B) & (offs_d[None, :] < D)\n",
    "        do_mask = (offs_b[:, None] < B - b * BLOCK_SIZE_B) & (offs_d[None, :] < D)\n",
    "\n",
    "        x = tl.load(x_ptrs, mask=x_mask, other=0.0)                     # BLOCK_SIZE_B, D\n",
    "        do = tl.load(do_ptrs, mask=do_mask, other=0.0)                  # BLOCK_SIZE_B, D\n",
    "\n",
    "        z = tl.dot(x, w1, out_dtype=tl.float32)                         # BLOCK_SIZE_B, BLOCK_SIZE_E\n",
    "        # activation\n",
    "        if ACTIVATION == \"leaky_relu\":\n",
    "            h = leaky_relu(z).to(TARGET_TYPE)                           # BLOCK_SIZE_B, BLOCK_SIZE_E\n",
    "        else:\n",
    "            h = z.to(TARGET_TYPE)                                       # BLOCK_SIZE_B, BLOCK_SIZE_E\n",
    "\n",
    "        dw2 += tl.dot(tl.trans(h), do, out_dtype=tl.float32)            # BLOCK_SIZE_E, D\n",
    "        dh = tl.dot(do, tl.trans(w2), out_dtype=tl.float32)             # BLOCK_SIZE_B, BLOCK_SIZE_E\n",
    "\n",
    "        if ACTIVATION == \"leaky_relu\":\n",
    "            dz = (dh * d_leacky_relu_inv_backward(z)).to(TARGET_TYPE)   # BLOCK_SIZE_B, BLOCK_SIZE_E\n",
    "        else:\n",
    "            dz = dh.to(TARGET_TYPE)\n",
    "\n",
    "        dw1 += tl.dot(tl.trans(x), dz, out_dtype=tl.float32)            # D, BLOCK_SIZE_E\n",
    "\n",
    "        # dx = tl.load(dx_ptrs, eviction_policy=\"evict_last\")\n",
    "        # dx += tl.dot(dz, tl.trans(w1), out_dtype=tl.float32)          # BLOCK_SIZE_B, D\n",
    "        # tl.store(dx_ptrs, dx.to(TARGET_TYPE),eviction_policy=\"evict_last\")\n",
    "\n",
    "        dx = tl.dot(dz, tl.trans(w1), out_dtype=tl.float32)             # BLOCK_SIZE_B, D\n",
    "        tl.atomic_add(dx_ptrs, dx.to(TARGET_TYPE), mask=dx_mask)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        dx_ptrs += BLOCK_SIZE_B * stride_dxb\n",
    "        do_ptrs += BLOCK_SIZE_B * stride_dob                \n",
    "        x_ptrs += BLOCK_SIZE_B * stride_xb\n",
    "\n",
    "    tl.store(dw1_ptrs, dw1.to(TARGET_TYPE), mask=dw1_mask)\n",
    "    tl.store(dw2_ptrs, dw2.to(TARGET_TYPE), mask=dw2_mask)\n",
    "    \"\"\"\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_wide_triton_bwd1(x, w1, w2, o, do, activation=\"\"):\n",
    "    # Check constraints.\n",
    "    assert x.shape[1] == w1.shape[0], \"Incompatible dimensions\"\n",
    "    assert w1.shape[1] == w2.shape[0], \"Incompatible dimensions\"\n",
    "    assert x.shape[1] == w2.shape[1], \"Incompatible dimensions\"\n",
    "    assert x.shape[0] == o.shape[0], \"Incompatible dimensions\"\n",
    "    assert x.shape[0] == do.shape[0], \"Incompatible dimensions\"\n",
    "    assert o.shape[0] == do.shape[0], \"Incompatible dimensions\"\n",
    "    assert o.shape[1] == do.shape[1], \"Incompatible dimensions\"\n",
    "    assert x.is_contiguous(), \"Matrix X must be contiguous\"\n",
    "    assert w1.is_contiguous(), \"Matrix W1 must be contiguous\"\n",
    "    assert w2.is_contiguous(), \"Matrix W2 must be contiguous\"\n",
    "    assert o.is_contiguous(), \"Matrix O must be contiguous\"\n",
    "    assert do.is_contiguous(), \"Matrix dO must be contiguous\"\n",
    "\n",
    "    B, D = x.shape\n",
    "    E = w1.shape[-1]\n",
    "\n",
    "    # Allocates output.\n",
    "    dx = torch.zeros_like(x)\n",
    "    dw1 = torch.zeros_like(w1)\n",
    "    dw2 = torch.zeros_like(w2)\n",
    "    # print(dx.shape, dw1.shape, dw2.shape, do.shape)\n",
    "\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(E, META['BLOCK_SIZE_E']),\n",
    "    )\n",
    "    mlp_wide_kernel_bwd1[grid](\n",
    "        x, w1, w2, o, dx, dw1, dw2, do,\n",
    "        B, D, E,\n",
    "        x.stride(0), x.stride(1),\n",
    "        w1.stride(0), w1.stride(1),\n",
    "        w2.stride(0), w2.stride(1),\n",
    "        o.stride(0), o.stride(1),\n",
    "        dx.stride(0), dx.stride(1),\n",
    "        dw1.stride(0), dw1.stride(1),\n",
    "        dw2.stride(0), dw2.stride(1),\n",
    "        do.stride(0), do.stride(1),\n",
    "        ACTIVATION=activation\n",
    "    )\n",
    "\n",
    "    # print(dx.shape, dw1.shape, dw2.shape)\n",
    "    return dx, dw1, dw2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_torch_bwd(x, w1, w2, o, do, activation=\"\"):\n",
    "    # x: B, D\n",
    "    # w1: D, E\n",
    "    # w2: E, D\n",
    "    # o: B, D\n",
    "    # do: B, D\n",
    "    z = torch.matmul(x, w1) # B, E\n",
    "    h = z\n",
    "    if activation == \"leaky_relu\":\n",
    "        h = torch.nn.functional.leaky_relu(z, negative_slope=0.01)\n",
    "    \n",
    "    dh = torch.matmul(do, torch.transpose(w2, 0, 1)) # B, E\n",
    "    dw2 = torch.matmul(torch.transpose(h, 0, 1), do) # E, D\n",
    "\n",
    "    dz = dh\n",
    "    if activation == \"leaky_relu\":\n",
    "        dz = (dh * torch.where(z >= 0, 1.0, 0.01)).to(dh.dtype)  # B, E\n",
    "\n",
    "    dx = torch.matmul(dz, torch.transpose(w1, 0, 1)) # B, D\n",
    "    dw1 = torch.matmul(torch.transpose(x, 0, 1), dz) # D, E\n",
    "\n",
    "    return dx, dw1, dw2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton_output=(torch.Size([4096, 64]), tensor([[-0.0450,  0.3645,  0.0683,  ..., -0.1090, -0.0099,  0.1100],\n",
      "        [-0.2422,  0.2617, -0.0426,  ...,  0.1908,  0.2352,  0.1393],\n",
      "        [-0.3655,  0.1578,  0.1257,  ..., -0.1119,  0.0750,  0.1035],\n",
      "        ...,\n",
      "        [-0.2080, -0.0995, -0.0374,  ...,  0.1044,  0.1842, -0.0544],\n",
      "        [-0.0833,  0.0177, -0.1833,  ...,  0.0799, -0.0060,  0.0704],\n",
      "        [-0.0994,  0.2700, -0.1600,  ...,  0.2289,  0.3455, -0.2137]],\n",
      "       device='cuda:0', dtype=torch.float16))\n",
      "torch_output=(torch.Size([4096, 64]), tensor([[-0.0450,  0.3645,  0.0683,  ..., -0.1090, -0.0099,  0.1100],\n",
      "        [-0.2422,  0.2617, -0.0426,  ...,  0.1908,  0.2352,  0.1393],\n",
      "        [-0.3655,  0.1578,  0.1257,  ..., -0.1119,  0.0750,  0.1035],\n",
      "        ...,\n",
      "        [-0.2080, -0.0995, -0.0374,  ...,  0.1044,  0.1842, -0.0544],\n",
      "        [-0.0833,  0.0177, -0.1833,  ...,  0.0799, -0.0060,  0.0704],\n",
      "        [-0.0994,  0.2700, -0.1600,  ...,  0.2289,  0.3455, -0.2137]],\n",
      "       device='cuda:0', dtype=torch.float16))\n",
      "✅ Triton and Torch match\n",
      "[dx: torch.Size([4096, 64]), torch.Size([4096, 64])] max diff: 1.46e-03, mean diff: 4.81e-05, rel max diff: 4773.36%, rel mean diff: 0.21%\n",
      "[dw1: torch.Size([64, 768]), torch.Size([64, 768])] max diff: 6.29e-03, mean diff: 5.50e-06, rel max diff: 8.04%, rel mean diff: 0.00%\n",
      "[dw2: torch.Size([768, 64]), torch.Size([768, 64])] max diff: 4.88e-04, mean diff: 1.03e-06, rel max diff: 227.30%, rel mean diff: 0.01%\n"
     ]
    }
   ],
   "source": [
    "def unit_test_bwd1():\n",
    "    # torch.manual_seed(17)\n",
    "    dtype = torch.float16\n",
    "    B = 1024 * 4\n",
    "    D = 64\n",
    "    E = 768\n",
    "    H = 1\n",
    "    x = torch.randn((B, D), device='cuda', dtype=dtype) / np.sqrt(D)\n",
    "    w1 = torch.randn((D, E), device='cuda', dtype=dtype) / np.sqrt(E)\n",
    "    w2 = torch.randn((E, D), device='cuda', dtype=dtype) / np.sqrt(D)\n",
    "    o = torch.randn((B, D), device='cuda', dtype=dtype) / np.sqrt(D)\n",
    "    do = torch.randn((B, D), device='cuda', dtype=dtype) / np.sqrt(D)\n",
    "    \n",
    "    triton_output = mlp_wide_triton_bwd1(x, w1, w2, o, do, activation=\"leaky_relu\")\n",
    "    torch_output = mlp_torch_bwd(x, w1, w2, o, do, activation=\"leaky_relu\")\n",
    "\n",
    "    print(f\"triton_output={triton_output[0].shape, triton_output[2]}\")\n",
    "    print(f\"torch_output={torch_output[0].shape, torch_output[2]}\")\n",
    "\n",
    "    eplison = 2e-2\n",
    "    if torch.allclose(triton_output[0], torch_output[0], atol=eplison, rtol=eplison):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")\n",
    "\n",
    "    dx_diff = np.abs(triton_output[0].to(torch.float32).cpu().numpy() - torch_output[0].to(torch.float32).cpu().numpy())\n",
    "    dx_rel_diff = dx_diff / np.abs(triton_output[0].to(torch.float32).cpu().numpy() + torch_output[0].to(torch.float32).cpu().numpy() + eplison)\n",
    "    print(f\"[dx: {triton_output[0].shape}, {torch_output[0].shape}] max diff: {np.max(dx_diff):.2e}, mean diff: {np.mean(dx_diff):.2e}, rel max diff: {np.max(dx_rel_diff)*100:.2f}%, rel mean diff: {np.mean(dx_rel_diff)*100:.2f}%\")\n",
    "\n",
    "    dw1_diff = np.abs(triton_output[1].to(torch.float32).cpu().numpy() - torch_output[1].to(torch.float32).cpu().numpy())\n",
    "    dw1_rel_diff = dw1_diff / np.abs(triton_output[1].to(torch.float32).cpu().numpy() + torch_output[1].to(torch.float32).cpu().numpy() + eplison)\n",
    "    print(f\"[dw1: {triton_output[1].shape}, {torch_output[1].shape}] max diff: {np.max(dw1_diff):.2e}, mean diff: {np.mean(dw1_diff):.2e}, rel max diff: {np.max(dw1_rel_diff)*100:.2f}%, rel mean diff: {np.mean(dw1_rel_diff)*100:.2f}%\")\n",
    "\n",
    "    dw2_diff = np.abs(triton_output[2].to(torch.float32).cpu().numpy() - torch_output[2].to(torch.float32).cpu().numpy())\n",
    "    dw2_rel_diff = dw2_diff / np.abs(triton_output[2].to(torch.float32).cpu().numpy() + torch_output[2].to(torch.float32).cpu().numpy() + eplison)\n",
    "    print(f\"[dw2: {triton_output[2].shape}, {torch_output[2].shape}] max diff: {np.max(dw2_diff):.2e}, mean diff: {np.mean(dw2_diff):.2e}, rel max diff: {np.max(dw2_rel_diff)*100:.2f}%, rel mean diff: {np.mean(dw2_rel_diff)*100:.2f}%\")\n",
    "\n",
    "unit_test_bwd1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['E'],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[\n",
    "            # 2 ** i for i in range(5, 12)\n",
    "            128 * i for i in range(2, 20)\n",
    "        ],  # Different possible values for `x_name`\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=['torch-16', 'triton-16', 'torch-32', 'triton-32', 'torch-64', 'triton-64', 'torch-128', 'triton-128'],\n",
    "        # Label name for the lines\n",
    "        line_names=[\"Torch-16\", \"Triton-16\", \"Torch-32\", \"Triton-32\", \"Torch-64\", \"Triton-64\", \"Torch-128\", \"Triton-128\"],\n",
    "        # Line styles\n",
    "        styles=[('green', ':'), ('blue', ':'), ('green', '--'), ('blue', '--'), ('green', '-.'), ('blue', '-.'), ('green', '-'), ('blue', '-')],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        plot_name=\"mlp-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    )\n",
    ")\n",
    "def benchmark_bwd1(E, provider):\n",
    "    dtype = torch.float16\n",
    "    D = int(provider[provider.find('-') + 1:])\n",
    "    HEAD = 768 // D\n",
    "    L = 1024\n",
    "    S = HEAD * 4\n",
    "    B = L * S\n",
    "\n",
    "    x = torch.randn((B, D), device='cuda', dtype=dtype) / np.sqrt(D)\n",
    "    w1 = torch.randn((D, E), device='cuda', dtype=dtype) / np.sqrt(E)\n",
    "    w2 = torch.randn((E, D), device='cuda', dtype=dtype) / np.sqrt(D)\n",
    "    o = torch.randn((B, D), device='cuda', dtype=dtype) / np.sqrt(D)\n",
    "    do = torch.randn((B, D), device='cuda', dtype=dtype) / np.sqrt(D)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider.startswith('torch'):\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: mlp_torch_bwd(x, w1, w2, o, do, activation=\"leaky_relu\"), quantiles=quantiles)\n",
    "    if provider.startswith('triton'):\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: mlp_wide_triton_bwd1(x, w1, w2, o, do, activation=\"leaky_relu\"), quantiles=quantiles)\n",
    "    perf = lambda ms: 8 * B * D * E * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_bwd1.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
