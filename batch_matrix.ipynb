{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n",
    "@triton.jit\n",
    "def leaky_relu(x):\n",
    "    x = x + 1\n",
    "    return tl.where(x >= 0, x, 0.01 * x)\n",
    "\n",
    "# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n",
    "#   - A list of `triton.Config` objects that define different configurations of\n",
    "#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n",
    "#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n",
    "#       provided configs\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    # Pointers to matrices\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    # Matrix dimensions\n",
    "    B, M, N, K,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_ab, stride_am, stride_ak,\n",
    "    stride_bb, stride_bk, stride_bn,\n",
    "    stride_cb, stride_cm, stride_cn,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr,\n",
    "    ACTIVATION: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    offs_b = tl.program_id(axis=1)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + (pid % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetics` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_b * stride_ab + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_b * stride_bb + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    # You can fuse arbitrary activation functions here\n",
    "    # while the accumulator is still in FP32!\n",
    "    if ACTIVATION == \"leaky_relu\":\n",
    "        accumulator = leaky_relu(accumulator)\n",
    "    c = accumulator.to(tl.bfloat16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + (offs_b * stride_cb + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :])\n",
    "    c_mask = (offs_b < B) & (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a, b, activation=\"\"):\n",
    "    # Check constraints.\n",
    "    assert a.shape[0] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.shape[2] == b.shape[1], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n",
    "    B, M, K = a.shape\n",
    "    B, K, N = b.shape\n",
    "\n",
    "    # Allocates output.\n",
    "    c = torch.empty((B, M, N), device=a.device, dtype=a.dtype)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n",
    "        B,\n",
    "    )\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        B, M, N, K,\n",
    "        a.stride(0), a.stride(1), a.stride(2),\n",
    "        b.stride(0), b.stride(1), b.stride(2),\n",
    "        c.stride(0), c.stride(1), c.stride(2),\n",
    "        ACTIVATION=activation\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton_output=tensor([[[ 49.0000,  14.9375,   1.0703,  ...,  15.6875, -33.0000, -10.3125],\n",
      "         [-30.6250,   0.7773,  -4.6875,  ..., -31.5000,  37.5000,  10.2500],\n",
      "         [-19.6250,   7.4062, -36.5000,  ...,   1.2734,   9.0000,  35.5000],\n",
      "         ...,\n",
      "         [-27.5000, -12.1250,  17.8750,  ...,  31.5000,  31.5000,  34.2500],\n",
      "         [ 16.1250, -15.5625,  -5.5000,  ...,  23.3750,  18.8750, -23.2500],\n",
      "         [-49.2500,  11.6875,  12.7500,  ...,  48.5000,  25.2500,  50.2500]],\n",
      "\n",
      "        [[ 41.5000, -32.2500,  15.1250,  ...,  12.5625, -18.7500,   2.2031],\n",
      "         [-23.7500, -49.5000, -26.1250,  ..., -35.5000,   1.1719, -33.0000],\n",
      "         [  7.0000,   2.2031,  -6.9062,  ...,  14.6250, -33.2500,  49.7500],\n",
      "         ...,\n",
      "         [ 38.0000, -13.3125, -25.6250,  ...,  17.3750, -25.8750,   3.6250],\n",
      "         [  4.3438,  -2.4375,   9.2500,  ..., -16.5000,   2.6875,   1.2188],\n",
      "         [-20.2500,  36.2500,  15.8750,  ...,  -9.3125,  10.6875,  43.2500]],\n",
      "\n",
      "        [[ -3.5781, -22.3750, -36.7500,  ...,  11.5625,   7.1562, -24.5000],\n",
      "         [ 40.5000, -31.7500,  -6.0312,  ..., -17.5000, -10.8125, -13.9375],\n",
      "         [-22.2500,  14.5000,  -8.7500,  ...,  19.5000,  -8.0000,   6.3750],\n",
      "         ...,\n",
      "         [ 21.5000,  -6.6250,   3.3125,  ..., -17.6250,  52.2500, -29.0000],\n",
      "         [ -4.0312,  22.5000, -28.2500,  ..., -20.8750, -34.7500, -10.3125],\n",
      "         [ 17.7500,  12.6875,  34.7500,  ...,  -4.1250,   1.8125,  -2.3594]],\n",
      "\n",
      "        [[-87.0000,  -0.7266, -10.6250,  ..., -20.1250,  -0.5352,   5.0625],\n",
      "         [-11.3125, -10.3125, -19.0000,  ...,   6.8438,  -2.3281, -10.3750],\n",
      "         [ -7.5000,  53.0000,  27.2500,  ...,   1.1250, -83.0000,   2.5000],\n",
      "         ...,\n",
      "         [-19.0000,  56.0000,  18.3750,  ...,  28.2500,  13.8125,  32.7500],\n",
      "         [ 42.0000,   5.9062,  16.8750,  ...,   0.4219,  13.0000,  -5.3438],\n",
      "         [-41.2500,  -5.6250,   9.1250,  ..., -27.5000,   2.1875, -15.1875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "torch_output=tensor([[[ 49.0000,  14.9375,   1.0703,  ...,  15.6875, -33.0000, -10.3125],\n",
      "         [-30.6250,   0.7773,  -4.6875,  ..., -31.5000,  37.5000,  10.2500],\n",
      "         [-19.6250,   7.4062, -36.5000,  ...,   1.2734,   9.0000,  35.5000],\n",
      "         ...,\n",
      "         [-27.5000, -12.1250,  17.8750,  ...,  31.5000,  31.5000,  34.2500],\n",
      "         [ 16.1250, -15.5625,  -5.5000,  ...,  23.3750,  18.8750, -23.2500],\n",
      "         [-49.2500,  11.6875,  12.7500,  ...,  48.5000,  25.2500,  50.2500]],\n",
      "\n",
      "        [[ 41.5000, -32.2500,  15.1250,  ...,  12.5625, -18.7500,   2.2031],\n",
      "         [-23.7500, -49.5000, -26.1250,  ..., -35.5000,   1.1719, -33.0000],\n",
      "         [  7.0000,   2.2031,  -6.9062,  ...,  14.6250, -33.2500,  49.7500],\n",
      "         ...,\n",
      "         [ 38.0000, -13.3125, -25.6250,  ...,  17.3750, -25.8750,   3.6250],\n",
      "         [  4.3438,  -2.4375,   9.2500,  ..., -16.5000,   2.6875,   1.2188],\n",
      "         [-20.2500,  36.2500,  15.8750,  ...,  -9.3125,  10.6875,  43.2500]],\n",
      "\n",
      "        [[ -3.5781, -22.3750, -36.7500,  ...,  11.5625,   7.1562, -24.5000],\n",
      "         [ 40.5000, -31.7500,  -6.0312,  ..., -17.5000, -10.8125, -13.9375],\n",
      "         [-22.2500,  14.5000,  -8.7500,  ...,  19.5000,  -8.0000,   6.3750],\n",
      "         ...,\n",
      "         [ 21.5000,  -6.6250,   3.3125,  ..., -17.6250,  52.2500, -29.0000],\n",
      "         [ -4.0312,  22.5000, -28.2500,  ..., -20.8750, -34.7500, -10.3125],\n",
      "         [ 17.7500,  12.6875,  34.7500,  ...,  -4.1250,   1.8125,  -2.3594]],\n",
      "\n",
      "        [[-87.0000,  -0.7266, -10.6250,  ..., -20.1250,  -0.5352,   5.0625],\n",
      "         [-11.3125, -10.3125, -19.0000,  ...,   6.8438,  -2.3281, -10.3750],\n",
      "         [ -7.5000,  53.0000,  27.2500,  ...,   1.1250, -83.0000,   2.5000],\n",
      "         ...,\n",
      "         [-19.0000,  56.0000,  18.3750,  ...,  28.2500,  13.8125,  32.7500],\n",
      "         [ 42.0000,   5.9062,  16.8750,  ...,   0.4219,  13.0000,  -5.3438],\n",
      "         [-41.2500,  -5.6250,   9.1250,  ..., -27.5000,   2.1875, -15.1875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "def unit_test():\n",
    "    torch.manual_seed(0)\n",
    "    a = torch.randn((4, 512, 512), device='cuda', dtype=torch.bfloat16)\n",
    "    b = torch.randn((4, 512, 512), device='cuda', dtype=torch.bfloat16)\n",
    "    triton_output = matmul(a, b)\n",
    "    torch_output = torch.matmul(a, b)\n",
    "    print(f\"triton_output={triton_output}\")\n",
    "    print(f\"torch_output={torch_output}\")\n",
    "    if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")\n",
    "\n",
    "unit_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[\n",
    "            128 * i for i in range(2, 36)\n",
    "        ],  # Different possible values for `x_name`\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=['cublas-1', 'triton-1', 'cublas-8', 'triton-8'],\n",
    "        # Label name for the lines\n",
    "        line_names=[\"cuBLAS-1\", \"Triton-1\", \"cuBLAS-8\", \"Triton-8\"],\n",
    "        # Line styles\n",
    "        styles=[('green', '--'), ('blue', '--'), ('green', ':'), ('blue', ':')],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    )\n",
    ")\n",
    "def benchmark(M, N, K, provider):\n",
    "    B = int(provider[-1:])\n",
    "    a = torch.randn((B, M//2, K*4), device='cuda', dtype=torch.bfloat16)\n",
    "    b = torch.randn((B, K*4, N//2), device='cuda', dtype=torch.bfloat16)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider.startswith('cublas'):\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.bmm(a, b), quantiles=quantiles)\n",
    "    if provider.startswith('triton'):\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * B * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 GiB. GPU 0 has a total capacity of 15.70 GiB of which 14.55 GiB is free. Process 480376 has 888.00 MiB memory in use. Including non-PyTorch memory, this process has 276.00 MiB memory in use. Of the allocated memory 8.12 MiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch20/lib/python3.12/site-packages/triton/testing.py:346\u001b[0m, in \u001b[0;36mMark.run\u001b[0;34m(self, show_plots, print_data, save_path, return_df, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m     html\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<html><body>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bench \u001b[38;5;129;01min\u001b[39;00m benchmarks:\n\u001b[0;32m--> 346\u001b[0m     result_dfs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbench\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m save_path:\n\u001b[1;32m    348\u001b[0m         html\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image src=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mbench\u001b[38;5;241m.\u001b[39mplot_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m/>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch20/lib/python3.12/site-packages/triton/testing.py:289\u001b[0m, in \u001b[0;36mMark._run\u001b[0;34m(self, bench, save_path, show_plots, print_data, diff_col, save_precision, **kwrags)\u001b[0m\n\u001b[1;32m    287\u001b[0m row_mean, row_min, row_max \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m bench\u001b[38;5;241m.\u001b[39mline_vals:\n\u001b[0;32m--> 289\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline_arg\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwrags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m         y_mean, y_min, y_max \u001b[38;5;241m=\u001b[39m ret\n",
      "Cell \u001b[0;32mIn[19], line 21\u001b[0m, in \u001b[0;36mbenchmark\u001b[0;34m(M, N, K, provider)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@triton\u001b[39m\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39mperf_report(\n\u001b[1;32m      2\u001b[0m     triton\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39mBenchmark(\n\u001b[1;32m      3\u001b[0m         x_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Argument names to use as an x-axis for the plot\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbenchmark\u001b[39m(M, N, K, provider):\n\u001b[1;32m     20\u001b[0m     B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(provider[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 21\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((B, K\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m, N\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m     23\u001b[0m     quantiles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.8\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 GiB. GPU 0 has a total capacity of 15.70 GiB of which 14.55 GiB is free. Process 480376 has 888.00 MiB memory in use. Including non-PyTorch memory, this process has 276.00 MiB memory in use. Of the allocated memory 8.12 MiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "benchmark.run(show_plots=True, print_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
