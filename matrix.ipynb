{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n",
    "@triton.jit\n",
    "def leaky_relu(x):\n",
    "    x = x + 1\n",
    "    return tl.where(x >= 0, x, 0.01 * x)\n",
    "\n",
    "# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n",
    "#   - A list of `triton.Config` objects that define different configurations of\n",
    "#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n",
    "#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n",
    "#       provided configs\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        # triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    # Pointers to matrices\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    # Matrix dimensions\n",
    "    M, N, K,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr,\n",
    "    ACTIVATION: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + (pid % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetics` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    # You can fuse arbitrary activation functions here\n",
    "    # while the accumulator is still in FP32!\n",
    "    if ACTIVATION == \"leaky_relu\":\n",
    "        accumulator = leaky_relu(accumulator)\n",
    "    c = accumulator.to(tl.bfloat16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a, b, activation=\"\"):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n",
    "    )\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        ACTIVATION=activation\n",
    "    )\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton_output=tensor([[-5.7000e+01, -7.6875e+00, -2.6875e+01,  ...,  3.2188e+00,\n",
      "          1.3812e+01, -3.5000e+01],\n",
      "        [ 2.7125e+01,  1.0500e+01,  1.9219e+00,  ...,  2.2750e+01,\n",
      "         -1.3938e+01,  4.5312e-01],\n",
      "        [-1.8672e+00,  1.8000e+01, -4.1750e+01,  ...,  1.5438e+01,\n",
      "         -3.7750e+01,  2.5250e+01],\n",
      "        ...,\n",
      "        [ 1.3750e+01, -1.2000e+01, -3.3000e+01,  ..., -1.5938e+01,\n",
      "          2.7812e+00,  2.6625e+01],\n",
      "        [-4.7607e-02,  1.2250e+01, -1.5438e+01,  ..., -7.4062e+00,\n",
      "         -7.3125e+00, -5.2500e+00],\n",
      "        [ 2.8000e+01,  3.1375e+01, -1.9125e+01,  ..., -1.3750e+01,\n",
      "          2.3625e+01,  1.1812e+01]], device='cuda:0', dtype=torch.bfloat16)\n",
      "torch_output=tensor([[-5.7000e+01, -7.6875e+00, -2.6875e+01,  ...,  3.2188e+00,\n",
      "          1.3812e+01, -3.5000e+01],\n",
      "        [ 2.7125e+01,  1.0500e+01,  1.9219e+00,  ...,  2.2750e+01,\n",
      "         -1.3938e+01,  4.5312e-01],\n",
      "        [-1.8672e+00,  1.8000e+01, -4.1750e+01,  ...,  1.5438e+01,\n",
      "         -3.7750e+01,  2.5250e+01],\n",
      "        ...,\n",
      "        [ 1.3750e+01, -1.2000e+01, -3.3000e+01,  ..., -1.5938e+01,\n",
      "          2.7812e+00,  2.6625e+01],\n",
      "        [-4.7607e-02,  1.2250e+01, -1.5438e+01,  ..., -7.4062e+00,\n",
      "         -7.3125e+00, -5.2500e+00],\n",
      "        [ 2.8000e+01,  3.1375e+01, -1.9125e+01,  ..., -1.3750e+01,\n",
      "          2.3625e+01,  1.1812e+01]], device='cuda:0', dtype=torch.bfloat16)\n",
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "def unit_test():\n",
    "    torch.manual_seed(0)\n",
    "    a = torch.randn((512, 512), device='cuda', dtype=torch.bfloat16)\n",
    "    b = torch.randn((512, 512), device='cuda', dtype=torch.bfloat16)\n",
    "    triton_output = matmul(a, b)\n",
    "    torch_output = torch.matmul(a, b)\n",
    "    print(f\"triton_output={triton_output}\")\n",
    "    print(f\"torch_output={torch_output}\")\n",
    "    if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")\n",
    "\n",
    "unit_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[\n",
    "            128 * i for i in range(2, 36)\n",
    "        ],  # Different possible values for `x_name`\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=['cublas', 'triton'],\n",
    "        # Label name for the lines\n",
    "        line_names=[\"cuBLAS\", \"Triton\"],\n",
    "        # Line styles\n",
    "        styles=[('green', '-'), ('blue', '-')],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    )\n",
    ")\n",
    "def benchmark(M, N, K, provider):\n",
    "    a = torch.randn((M, K), device='cuda', dtype=torch.bfloat16)\n",
    "    b = torch.randn((K, N), device='cuda', dtype=torch.bfloat16)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'cublas':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matmul-performance:\n",
      "         M       N       K     cuBLAS     Triton\n",
      "0    256.0   256.0   256.0   4.096000   4.681143\n",
      "1    384.0   384.0   384.0  13.824000  12.288000\n",
      "2    512.0   512.0   512.0  26.214401  20.164923\n",
      "3    640.0   640.0   640.0  36.571428  32.000000\n",
      "4    768.0   768.0   768.0  36.863999  38.466782\n",
      "5    896.0   896.0   896.0  52.034370  52.034370\n",
      "6   1024.0  1024.0  1024.0  55.188213  69.399030\n",
      "7   1152.0  1152.0  1152.0  56.339321  57.422770\n",
      "8   1280.0  1280.0  1280.0  61.134328  60.235293\n",
      "9   1408.0  1408.0  1408.0  72.690343  72.690343\n",
      "10  1536.0  1536.0  1536.0  64.934752  67.408458\n",
      "11  1664.0  1664.0  1664.0  68.173575  71.349154\n",
      "12  1792.0  1792.0  1792.0  78.597372  82.039594\n",
      "13  1920.0  1920.0  1920.0  72.787823  77.662920\n",
      "14  2048.0  2048.0  2048.0  83.010581  81.840076\n",
      "15  2176.0  2176.0  2176.0  76.807821  77.998640\n",
      "16  2304.0  2304.0  2304.0  80.430548  82.657000\n",
      "17  2432.0  2432.0  2432.0  78.257560  80.669642\n",
      "18  2560.0  2560.0  2560.0  75.155963  74.962537\n",
      "19  2688.0  2688.0  2688.0  78.536346  77.335485\n",
      "20  2816.0  2816.0  2816.0  76.248617  75.326786\n",
      "21  2944.0  2944.0  2944.0  74.160762  72.859693\n",
      "22  3072.0  3072.0  3072.0  79.193155  79.526831\n",
      "23  3200.0  3200.0  3200.0  80.301128  77.909231\n",
      "24  3328.0  3328.0  3328.0  76.505096  75.462575\n",
      "25  3456.0  3456.0  3456.0  78.425648  76.491052\n",
      "26  3584.0  3584.0  3584.0  80.067131  80.641608\n",
      "27  3712.0  3712.0  3712.0  78.535646  77.680671\n",
      "28  3840.0  3840.0  3840.0  78.322944  79.107293\n",
      "29  3968.0  3968.0  3968.0  77.524737  78.013403\n",
      "30  4096.0  4096.0  4096.0  76.959706  76.739694\n",
      "31  4224.0  4224.0  4224.0  77.759088  77.690469\n",
      "32  4352.0  4352.0  4352.0  76.876591  77.998640\n",
      "33  4480.0  4480.0  4480.0  77.969254  77.261766\n"
=======
      "         M      cuBLAS      Triton\n",
      "0    256.0    5.461333    6.096372\n",
      "1    384.0   12.461070   13.987921\n",
      "2    512.0   23.831273   26.214401\n",
      "3    640.0   46.545454   39.384616\n",
      "4    768.0   55.731399   52.043293\n",
      "5    896.0   73.943582   66.901334\n",
      "6   1024.0   67.108861   87.381330\n",
      "7   1152.0   96.322066  107.724338\n",
      "8   1280.0   99.902441  132.731146\n",
      "9   1408.0  121.150576  159.905431\n",
      "10  1536.0  124.173471  128.688872\n",
      "11  1664.0  132.094117  152.523928\n",
      "12  1792.0  111.281428  181.281035\n",
      "13  1920.0  131.657144  209.454544\n",
      "14  2048.0  149.796569  233.016881\n",
      "15  2176.0  134.353587  197.290670\n",
      "16  2304.0  143.041145  219.154788\n",
      "17  2432.0  143.156508  240.123617\n",
      "18  2560.0  145.635558  213.146861\n",
      "19  2688.0  155.204931  231.299126\n",
      "20  2816.0  141.604577  246.407963\n",
      "21  2944.0  146.603508  236.189718\n",
      "22  3072.0  160.405389  246.187403\n",
      "23  3200.0  161.145643  260.162591\n",
      "24  3328.0  164.445821  245.704090\n",
      "25  3456.0  150.694520  260.911227\n",
      "26  3584.0  149.113421  255.464138\n",
      "27  3712.0  149.996010  259.473626\n",
      "28  3840.0  148.445632  258.392524\n",
      "29  3968.0  164.231405  259.798128\n",
      "30  4096.0  167.621567  269.040805\n",
      "31  4224.0  163.735212  257.339081\n",
      "32  4352.0  148.698343  263.916700\n",
      "33  4480.0  164.718829  264.084211\n"
     ]
    }
   ],
   "source": [
    "benchmark.run(show_plots=True, print_data=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
